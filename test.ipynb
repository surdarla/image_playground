{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.cifar10.plcifar10 import CIFAR10Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': 'config',\n",
       "              '__doc__': '_summary_',\n",
       "              'seed': 43,\n",
       "              'data_dir': './data/cifar10',\n",
       "              'pth_dir': './pth',\n",
       "              'fold': 5,\n",
       "              'n_split': 5,\n",
       "              'num_workers': 2,\n",
       "              'MODEL': 'final',\n",
       "              'amp': True,\n",
       "              'print_freq': 100,\n",
       "              'batch_size': 128,\n",
       "              'image_size': 128,\n",
       "              'epochs': 10,\n",
       "              'patience': 3,\n",
       "              'accum_iter': 1,\n",
       "              'max_grad_norm': 1000,\n",
       "              'max_lr': 0.01,\n",
       "              'min_lr': 1e-05,\n",
       "              'weight_decay': 1e-06,\n",
       "              'augmix': 1,\n",
       "              'tta': 3,\n",
       "              'weights': [1],\n",
       "              '__dict__': <attribute '__dict__' of 'CFG' objects>,\n",
       "              '__weakref__': <attribute '__weakref__' of 'CFG' objects>})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = CIFAR10Data(CFG.data_dir, CFG.batch_size, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar.prepare_data()\n",
    "cifar.setup(stage='fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.alexnet import AlexNet, AlexNetLit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-05"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.min_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNetLit(10,lr=CFG.min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msurdarla\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/aljam/projects/image_playground/wandb/run-20220624_151043-2gl8sau5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/surdarla/image_playground/runs/2gl8sau5\" target=\"_blank\">test1</a></strong> to <a href=\"https://wandb.ai/surdarla/image_playground\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(name='test1',project='image_playground')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "pl.seed_everything(43)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=50,\n",
    "    callbacks=[EarlyStopping(monitor=\"VALIDATION LOSS\", mode=\"min\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | AlexNet          | 56.8 M\n",
      "1 | loss      | CrossEntropyLoss | 0     \n",
      "2 | train_acc | Accuracy         | 0     \n",
      "3 | valid_acc | Accuracy         | 0     \n",
      "4 | test_acc  | Accuracy         | 0     \n",
      "-----------------------------------------------\n",
      "56.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "56.8 M    Total params\n",
      "227.307   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca21d8cb74943b18ec1135b6d088052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aljam/opt/anaconda3/envs/image_play/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model,cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': 'config',\n",
       "              '__doc__': '_summary_',\n",
       "              'seed': 43,\n",
       "              'data_dir': './data/cifar10',\n",
       "              'pth_dir': './pth',\n",
       "              'fold': 5,\n",
       "              'n_split': 5,\n",
       "              'num_workers': 2,\n",
       "              'MODEL': 'final',\n",
       "              'amp': True,\n",
       "              'print_freq': 100,\n",
       "              'batch_size': 128,\n",
       "              'image_size': 128,\n",
       "              'epochs': 10,\n",
       "              'patience': 3,\n",
       "              'accum_iter': 1,\n",
       "              'max_grad_norm': 1000,\n",
       "              'max_lr': 0.01,\n",
       "              'min_lr': 1e-05,\n",
       "              'weight_decay': 1e-06,\n",
       "              'augmix': 1,\n",
       "              'tta': 3,\n",
       "              'weights': [1],\n",
       "              '__dict__': <attribute '__dict__' of 'CFG' objects>,\n",
       "              '__weakref__': <attribute '__weakref__' of 'CFG' objects>})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config import CFG\n",
    "CFG.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union\n",
    "cfgs: Dict[str, List[Union[str, int]]] = {\n",
    "    'VGG11' : [64, 'M', 128, 'M', 256, 256, 'M', 512,512, 'M',512,512,'M'],\n",
    "    'VGG13' : [64,64, 'M', 128, 128, 'M', 256, 256, 'M', 512,512, 'M', 512,512,'M'],\n",
    "    'VGG16' : [64,64, 'M', 128, 128, 'M', 256, 256,256, 'M', 512,512,512, 'M',512,512,512,'M'],\n",
    "    'VGG19' : [64,64, 'M', 128, 128, 'M', 256, 256,256,256, \\\n",
    "                        'M', 512,512,512,512, 'M',512,512,512,512,'M']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cfgs['VGG11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.vgg import VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG('VGG13',batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msurdarla\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/aljam/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/aljam/projects/image_playground/wandb/run-20220627_182947-1ebjilst</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/surdarla/image_playground/runs/1ebjilst\" target=\"_blank\">test1</a></strong> to <a href=\"https://wandb.ai/surdarla/image_playground\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type              | Params\n",
      "--------------------------------------------------\n",
      "0 | loss        | CrossEntropyLoss  | 0     \n",
      "1 | train_acc   | Accuracy          | 0     \n",
      "2 | valid_acc   | Accuracy          | 0     \n",
      "3 | test_acc    | Accuracy          | 0     \n",
      "4 | conv_layers | Sequential        | 20.0 M\n",
      "5 | avgpool     | AdaptiveAvgPool2d | 0     \n",
      "6 | classifier  | Sequential        | 119 M \n",
      "--------------------------------------------------\n",
      "139 M     Trainable params\n",
      "0         Non-trainable params\n",
      "139 M     Total params\n",
      "558.489   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aljam/opt/anaconda3/envs/image_play/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/aljam/opt/anaconda3/envs/image_play/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a80e4004ad4f38ba8b421c09f83126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aljam/opt/anaconda3/envs/image_play/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:726: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "/Users/aljam/opt/anaconda3/envs/image_play/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f31435ebe7445b9e1b4040916de607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "from data.cifar10.plcifar10 import CIFAR10Data\n",
    "from config import CFG\n",
    "# from model.alexnet import AlexNetLit\n",
    "from model.vgg import VGG\n",
    "\n",
    "wandb.login(key=CFG.wandb_key)\n",
    "cifar = CIFAR10Data(CFG.data_dir, CFG.batch_size, 0)\n",
    "cifar.prepare_data()\n",
    "cifar.setup(stage=\"fit\")\n",
    "model = VGG()\n",
    "wandb_logger = WandbLogger(name=\"test1\", project=\"image_playground\")\n",
    "pl.seed_everything(43)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"auto\",\n",
    "    auto_lr_find=True,\n",
    "    auto_scale_batch_size=True,\n",
    "    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=50,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"VALID LOSS\", mode=\"min\", patience=5, verbose=True),\n",
    "        LearningRateMonitor(logging_interval='step'),\n",
    "    ],\n",
    ")\n",
    "trainer.fit(model, cifar)\n",
    "trainer.test(model, cifar)\n",
    "trainer.save_checkpoint(CFG.pth_dir, f\"{CFG.MODEL}.pth\")\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "0d8cf6c5d06cf0d5adefa493a4932d80ab157bf6900285241bf9be2fac8513f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
